{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgxh5MlBRXu+F3b8uQG8oX"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBnXa7f97xMf"
      },
      "outputs": [],
      "source": [
        "from setuptools import setup, find_packages\n",
        "\n",
        "setup(\n",
        "\n",
        "    name='llm_agents',\n",
        "\n",
        "    version='0.1.0',\n",
        "\n",
        "    description='A package for building agents which use the OpenAI API to figure out actions to take and can use tools.',\n",
        "\n",
        "    author='Marc PÃ¤pper',\n",
        "\n",
        "    author_email='marc@paepper.com',\n",
        "\n",
        "    url='https://github.com/mpaepper/llm_agents',\n",
        "\n",
        "    packages=find_packages(),\n",
        "\n",
        "    install_requires=[\n",
        "\n",
        "        'google-search-results>=2.4.2',\n",
        "\n",
        "        'openai>=0.27.0',\n",
        "\n",
        "        'pydantic>=1.10.5',\n",
        "\n",
        "        'requests>=2.28.2'\n",
        "\n",
        "    ],\n",
        "\n",
        "    classifiers=[\n",
        "\n",
        "        'Programming Language :: Python :: 3',\n",
        "\n",
        "        'License :: OSI Approved :: MIT License'\n",
        "\n",
        "    ],\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llm_agents.agent import Agent\n",
        "\n",
        "from llm_agents.llm import ChatLLM\n",
        "\n",
        "from llm_agents.tools.python_repl import PythonREPLTool\n",
        "\n",
        "from llm_agents.tools.hackernews import HackerNewsSearchTool\n",
        "\n",
        "from llm_agents.tools.search import SerpAPITool\n",
        "\n",
        "from llm_agents.tools.searx import SearxSearchTool\n",
        "\n",
        "from llm_agents.tools.google_search import GoogleSearchTool\n",
        "\n",
        "__all__ = ['Agent', 'ChatLLM', 'PythonREPLTool',\n",
        "\n",
        "           'HackerNewsSearchTool', 'SerpAPITool', 'SearxSearchTool', 'GoogleSearchTool']"
      ],
      "metadata": {
        "id": "2GNa_5LX7793"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "import os\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from typing import List\n",
        "\n",
        "class ChatLLM(BaseModel):\n",
        "\n",
        "    model: str = 'gpt-3.5-turbo'\n",
        "\n",
        "    temperature: float = 0.0\n",
        "\n",
        "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]  # Credentials setup\n",
        "\n",
        "    def generate(self, prompt: str, stop: List[str] = None):\n",
        "\n",
        "        response = openai.ChatCompletion.create(\n",
        "\n",
        "            model=self.model,\n",
        "\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "\n",
        "            temperature=self.temperature,\n",
        "\n",
        "            stop=stop\n",
        "\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    llm = ChatLLM()\n",
        "\n",
        "    result = llm.generate(prompt='Who is the president of the USA?')\n",
        "\n",
        "    print(result)"
      ],
      "metadata": {
        "id": "g0vo1EFF7-z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "from io import StringIO\n",
        "\n",
        "from typing import Dict, Optional\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from llm_agents.tools.base import ToolInterface\n",
        "\n",
        "# Taken from https://github.com/hwchase17/langchain/blob/master/langchain/python.py\n",
        "\n",
        "class PythonREPL(BaseModel):\n",
        "\n",
        "    \"\"\"Simulates a standalone Python REPL.\"\"\"\n",
        "\n",
        "    globals: Optional[Dict] = Field(default_factory=dict, alias=\"_globals\")\n",
        "\n",
        "    locals: Optional[Dict] = Field(default_factory=dict, alias=\"_locals\")\n",
        "\n",
        "    def run(self, command: str) -> str:\n",
        "\n",
        "        \"\"\"Run command with own globals/locals and returns anything printed.\"\"\"\n",
        "\n",
        "        old_stdout = sys.stdout\n",
        "                sys.stdout = mystdout = StringIO()\n",
        "\n",
        "        try:\n",
        "\n",
        "            exec(command, self.globals, self.locals)\n",
        "\n",
        "            sys.stdout = old_stdout\n",
        "\n",
        "            output = mystdout.getvalue()\n",
        "\n",
        "        except Exception as e:\n",
        "\n",
        "            sys.stdout = old_stdout\n",
        "\n",
        "            output = str(e)\n",
        "\n",
        "        return output\n",
        "\n",
        "def _get_default_python_repl() -> PythonREPL:\n",
        "\n",
        "    return PythonREPL(_globals=globals(), _locals=None)\n",
        "\n",
        "class PythonREPLTool(ToolInterface):\n",
        "\n",
        "    \"\"\"A tool for running python code in a REPL.\"\"\"\n",
        "\n",
        "    name: str = \"Python REPL\"\n",
        "\n",
        "    description: str = (\n",
        "\n",
        "        \"A Python shell. Use this to execute python commands. \"\n",
        "\n",
        "        \"Input should be a valid python command. \"\n",
        "\n",
        "        \"If you want to see the output of a value, you should print it out \"\n",
        "\n",
        "        \"with `print(...)`.\"\n",
        "\n",
        "    )\n",
        "\n",
        "    python_repl: PythonREPL = Field(default_factory=_get_default_python_repl)\n",
        "\n",
        "    def use(self, input_text: str) -> str:\n",
        "\n",
        "        input_text = input_text.strip().strip(\"```\")\n",
        "\n",
        "        return self.python_repl.run(input_text)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    repl_tool = PythonREPLTool()\n",
        "\n",
        "    result = repl_tool.use('print(5 * 7)')\n",
        "\n",
        "    assert result == \"35\\n\"\n",
        "\n",
        "    print(result)"
      ],
      "metadata": {
        "id": "0foWG-lA8BKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "import re\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "from llm_agents.llm import ChatLLM\n",
        "\n",
        "from llm_agents.tools.base import ToolInterface\n",
        "\n",
        "from llm_agents.tools.python_repl import PythonREPLTool\n",
        "\n",
        "FINAL_ANSWER_TOKEN = \"Final Answer:\"\n",
        "\n",
        "OBSERVATION_TOKEN = \"Observation:\"\n",
        "\n",
        "THOUGHT_TOKEN = \"Thought:\"\n",
        "\n",
        "PROMPT_TEMPLATE = \"\"\"Today is {today} and you can use tools to get new information. Answer the question as best as you can using the following tools:\n",
        "\n",
        "{tool_description}\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "\n",
        "Thought: comment on what you want to do next\n",
        "\n",
        "Action: the action to take, exactly one element of [{tool_names}]\n",
        "\n",
        "Action Input: the input to the action\n",
        "\n",
        "Observation: the result of the action\n",
        "\n",
        "... (this Thought/Action/Action Input/Observation repeats N times, use it until you are sure of the answer)\n",
        "\n",
        "Thought: I now know the final answer\n",
        "\n",
        "Final Answer: your final answer to the original input question\n",
        "\n",
        "Begin!\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Thought: {previous_responses}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class Agent(BaseModel):\n",
        "\n",
        "    llm: ChatLLM\n",
        "\n",
        "    tools: List[ToolInterface]\n",
        "\n",
        "    prompt_template: str = PROMPT_TEMPLATE\n",
        "\n",
        "    max_loops: int = 15\n",
        "\n",
        "    # The stop pattern is used, so the LLM does not hallucinate until the end\n",
        "\n",
        "    stop_pattern: List[str] = [f'\\n{OBSERVATION_TOKEN}', f'\\n\\t{OBSERVATION_TOKEN}']\n",
        "\n",
        "    @property\n",
        "\n",
        "    def tool_description(self) -> str:\n",
        "\n",
        "        return \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
        "\n",
        "    @property\n",
        "\n",
        "    def tool_names(self) -> str:\n",
        "\n",
        "        return \",\".join([tool.name for tool in self.tools])\n",
        "\n",
        "    @property\n",
        "\n",
        "    def tool_by_names(self) -> Dict[str, ToolInterface]:\n",
        "\n",
        "        return {tool.name: tool for tool in self.tools}\n",
        "\n",
        "    def run(self, question: str):\n",
        "\n",
        "        previous_responses = []\n",
        "\n",
        "        num_loops = 0\n",
        "\n",
        "        prompt = self.prompt_template.format(\n",
        "\n",
        "                today = datetime.date.today(),\n",
        "\n",
        "                tool_description=self.tool_description,\n",
        "\n",
        "                tool_names=self.tool_names,\n",
        "\n",
        "                question=question,\n",
        "\n",
        "                previous_responses='{previous_responses}'\n",
        "\n",
        "        )\n",
        "\n",
        "        print(prompt.format(previous_responses=''))\n",
        "\n",
        "        while num_loops < self.max_loops:\n",
        "\n",
        "            num_loops += 1\n",
        "\n",
        "            curr_prompt = prompt.format(previous_responses='\\n'.join(previous_responses))\n",
        "\n",
        "            generated, tool, tool_input = self.decide_next_action(curr_prompt)\n",
        "\n",
        "            if tool == 'Final Answer':\n",
        "\n",
        "                return tool_input\n",
        "\n",
        "            if tool not in self.tool_by_names:\n",
        "\n",
        "                raise ValueError(f\"Unknown tool: {tool}\")\n",
        "\n",
        "            tool_result = self.tool_by_names[tool].use(tool_input)\n",
        "\n",
        "            generated += f\"\\n{OBSERVATION_TOKEN} {tool_result}\\n{THOUGHT_TOKEN}\"\n",
        "\n",
        "            print(generated)\n",
        "\n",
        "            previous_responses.append(generated)\n",
        "\n",
        "    def decide_next_action(self, prompt: str) -> str:\n",
        "\n",
        "        generated = self.llm.generate(prompt, stop=self.stop_pattern)\n",
        "\n",
        "        tool, tool_input = self._parse(generated)\n",
        "\n",
        "        return generated, tool, tool_input\n",
        "\n",
        "    def _parse(self, generated: str) -> Tuple[str, str]:\n",
        "\n",
        "        if FINAL_ANSWER_TOKEN in generated:\n",
        "\n",
        "            return \"Final Answer\", generated.split(FINAL_ANSWER_TOKEN)[-1].strip()\n",
        "\n",
        "        regex = r\"Action: [\\[]?(.*?)[\\]]?[\\n]*Action Input:[\\s]*(.*)\"\n",
        "\n",
        "        match = re.search(regex, generated, re.DOTALL)\n",
        "\n",
        "        if not match:\n",
        "\n",
        "            raise ValueError(f\"Output of LLM is not parsable for next tool use: `{generated}`\")\n",
        "\n",
        "        tool = match.group(1).strip()\n",
        "\n",
        "        tool_input = match.group(2)\n",
        "\n",
        "        return tool, tool_input.strip(\" \").strip('\"')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    agent = Agent(llm=ChatLLM(), tools=[PythonREPLTool()])\n",
        "\n",
        "    result = agent.run(\"What is 7 * 9 - 34 in Python?\")\n",
        "\n",
        "    print(f\"Final answer is {result}\")"
      ],
      "metadata": {
        "id": "EmOHm1oB8B1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llm_agents import Agent, ChatLLM, PythonREPLTool, HackerNewsSearchTool, SerpAPITool\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    prompt = input(\"Enter a question / task for the agent: \")\n",
        "\n",
        "    agent = Agent(llm=ChatLLM(), tools=[PythonREPLTool(), SerpAPITool(), HackerNewsSearchTool()])\n",
        "\n",
        "    result = agent.run(prompt)\n",
        "\n",
        "    print(f\"Final answer is {result}\")"
      ],
      "metadata": {
        "id": "ll8M0rEu8Kk7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}